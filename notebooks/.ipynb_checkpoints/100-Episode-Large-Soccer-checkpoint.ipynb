{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_name = \"../4th-Year-Project/Builds/Soccer-Large/4th-Year-Project\"  # Name of the Unity environment binary to launch\n",
    "train_mode = True  # Whether to run the environment in training or inference mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dependancies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python version:\n",
      "3.6.7 |Anaconda, Inc.| (default, Oct 28 2018, 19:44:12) [MSC v.1915 64 bit (AMD64)]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from mlagents.envs import UnityEnvironment\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Python version:\")\n",
    "print(sys.version)\n",
    "\n",
    "# check Python version\n",
    "if (sys.version_info[0] < 3):\n",
    "    raise Exception(\"ERROR: ML-Agents Toolkit (v0.3 onwards) requires Python 3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Start Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:mlagents.envs:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: Brain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space size (per agent): 8\n",
      "        Number of stacked Vector Observation: 3\n",
      "        Vector Action space type: continuous\n",
      "        Vector Action space size (per agent): [2]\n",
      "        Vector Action descriptions: , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=env_name)\n",
    "\n",
    "# Set the default brain to work with\n",
    "default_brain = env.brain_names[0]\n",
    "brain = env.brains[default_brain]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Restart Environment and view current state of the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent state looks like: \n",
      "[ 0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.          0.\n",
      "  0.          0.          0.          0.          0.61399996  0.108\n",
      "  0.38600001 -1.61399996  0.89200002 -1.10800004  0.          0.        ]\n"
     ]
    }
   ],
   "source": [
    "# Reset the environment\n",
    "env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "\n",
    "# Examine the state space for the default brain\n",
    "print(\"Agent state looks like: \\n{}\".format(env_info.vector_observations[0]))\n",
    "\n",
    "# Examine the observation space for the default brain\n",
    "for observation in env_info.visual_observations:\n",
    "    print(\"Agent observations look like:\")\n",
    "    if observation.shape[3] == 3:\n",
    "        plt.imshow(observation[0,:,:,:])\n",
    "    else:\n",
    "        plt.imshow(observation[0,:,:,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Take Random actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total reward this episode: -5.989999839104712\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.5100000156089664\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: -3.9749998841434717\n",
      "Total reward this episode: -100.79499772004783\n",
      "Total reward this episode: -3.4949998948723078\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.5750000141561031\n",
      "Total reward this episode: 0.5100000156089664\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: -10.969999727793038\n",
      "Total reward this episode: -14.4649996496737\n",
      "Total reward this episode: -8.974999772384763\n",
      "Total reward this episode: -46.399998935870826\n",
      "Total reward this episode: -4.9849998615682125\n",
      "Total reward this episode: -1.48999993968755\n",
      "Total reward this episode: 0.5100000156089664\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.015000026673078537\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.010000026784837246\n",
      "Total reward this episode: 0.005000026896595955\n",
      "Total reward this episode: -13.964999660849571\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.5150000154972076\n",
      "Total reward this episode: -18.45499956049025\n",
      "Total reward this episode: -16.459999605081975\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: -0.9799999510869384\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: -2.4799999175593257\n",
      "Total reward this episode: -12.964999683201313\n",
      "Total reward this episode: -17.459999582730234\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.4300000173971057\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.8450000081211329\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: -40.899999058805406\n",
      "Total reward this episode: -27.43999935965985\n",
      "Total reward this episode: -0.989999950863421\n",
      "Total reward this episode: -10.969999727793038\n",
      "Total reward this episode: -20.949999504722655\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: -132.22999701742083\n",
      "Total reward this episode: -11.469999716617167\n",
      "Total reward this episode: 0.010000026784837246\n",
      "Total reward this episode: -0.49499996192753315\n",
      "Total reward this episode: 0.5050000157207251\n",
      "Total reward this episode: 0.010000026784837246\n",
      "Total reward this episode: 0.9950000047683716\n",
      "Total reward this episode: 0.5150000154972076\n",
      "Total reward this episode: 0.005000026896595955\n",
      "Total reward this episode: 0.010000026784837246\n",
      "Total reward this episode: -291.9099934482947\n",
      "Total reward this episode: -31.92999925930053\n",
      "Total reward this episode: -6.4799998281523585\n",
      "Total reward this episode: -47.39499891363084\n",
      "Total reward this episode: -0.49499996192753315\n",
      "Total reward this episode: -5.979999839328229\n",
      "Total reward this episode: -239.5149946194142\n",
      "Total reward this episode: -5.4799998505041\n",
      "Total reward this episode: -17.459999582730234\n",
      "Total reward this episode: -25.439999404363334\n",
      "Total reward this episode: 0.46500001661479473\n"
     ]
    }
   ],
   "source": [
    "for episode in range(100):\n",
    "    env_info = env.reset(train_mode=train_mode)[default_brain]\n",
    "    done = False\n",
    "    episode_rewards = 0\n",
    "    while not done:\n",
    "        action_size = brain.vector_action_space_size\n",
    "        if brain.vector_action_space_type == 'continuous':\n",
    "            env_info = env.step(np.random.randn(len(env_info.agents), \n",
    "                                                action_size[0]))[default_brain]\n",
    "        else:\n",
    "            action = np.column_stack([np.random.randint(0, action_size[i], size=(len(env_info.agents))) for i in range(len(action_size))])\n",
    "            env_info = env.step(action)[default_brain]\n",
    "        episode_rewards += env_info.rewards[0]\n",
    "        done = env_info.local_done[0]\n",
    "    print(\"Total reward this episode: {}\".format(episode_rewards))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Close Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
